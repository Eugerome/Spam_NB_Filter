{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMS Spam Filter ###\n",
    "\n",
    "## Introduction ##\n",
    "\n",
    "In this notebook I will create a SMS spam filter using the Naive Bayes algorithm.\n",
    "\n",
    "## Import Data ##\n",
    "\n",
    "For this exercise I will use the UCL Spam Collection Data Set. The dataset is available [here](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection).\n",
    "After downloading the file we can load up the .csv file using pandas and take a look at the first few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# the encoding in this file is not utf-8, so we need to specify the encoding\n",
    "df = pd.read_csv(\"spam.csv\", encoding = \"latin-1\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two columns that are important in this dataframe. First - v1 - is the column that contains the __class__ of SMS and is what I will be teaching the model to predict. From the description of the dataset we know that there are only two __classes__: spam and ham (a common name for non-spam content).\n",
    "\n",
    "Second, column v2 which contains the __text__ of the SMS. This can be used to predict the __class__ of the SMS.\n",
    "\n",
    "To make it easier to work with the dataframe I will drop the unnecessary columns and rename v1 and v2 to __class__ and __text__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dropping all columns starting with unnamed, just listing the columns may have beed faster\n",
    "df.drop([c for c in df.columns.values.tolist() if c.startswith(\"Unnamed\")], axis = 1, inplace = True)\n",
    "df.rename(columns = {\"v1\" : \"class\", \"v2\" : \"text\"}, inplace = True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis ##\n",
    "\n",
    "Before proceeding with pre-processing our data so we can feed it to a model, it is a good idea to take a look at some features of our dataframe. To for this we can use the __describe__. We can look at the two classes (ham/spam)in more detail using the __groupby__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>4825</td>\n",
       "      <td>4516</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>747</td>\n",
       "      <td>653</td>\n",
       "      <td>Please call our customer service representativ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       text                                                               \n",
       "      count unique                                                top freq\n",
       "class                                                                     \n",
       "ham    4825   4516                             Sorry, I'll call later   30\n",
       "spam    747    653  Please call our customer service representativ...    4"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"class\").describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can see that the dataset is rather small. I can also see that there is some repetition in the the text, but it is not too bad. So to accurately predict the class of the text I would want a simple alogrithm, for example __Naive Bayes__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing ##\n",
    "\n",
    "Before I can feed the data to the algorithm, I need to clean up the text a bit. Initially we can rely on our knowledge of SMS and the examples we have seen so far. Here are 3 things we can consider:\n",
    "- Punctuation is not particularly consistent in SMS, so we should get rid of it.\n",
    "- As with any text, stopwords are generally unimportant to the meaining of the phrase, so we should get rid of them.\n",
    "- [Stemming](https://en.wikipedia.org/wiki/Stemming) (stripping the stem of a word) or [Lemmatization](https://en.wikipedia.org/wiki/Lemmatisation) (parsing the word to a certain form) is generally a good idea when working with text, since they can allow similar words by meaning to create identical vectors. However in our case this can be unreliable, since the text contains non-standart words like \"wkly\" (weekly), which makes it hard for standart __stemming__ and __lemmatizing__ methods to work reliably. So it is best to avoid them altogether for now.\n",
    "\n",
    "Now let's create a method that can clean our data the way we described. Thankfully __nltk__ contains a preset list of stopwords for English, and we can get punctuation symbols from python's __string__ module. We can also use __word_tokenize__ from nltk to split our sentences into separate __tokens__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "def clean_text(text):\n",
    "    text = word_tokenize(text)\n",
    "    text = [w for w in text if w not in stopwords.words(\"english\")]\n",
    "    text = [w for w in text if w not in string.punctuation]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the lines rows from __df.head()__ to test this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [Go, jurong, point, crazy.., Available, bugis,...\n",
      "1             [Ok, lar, ..., Joking, wif, u, oni, ...]\n",
      "2    [Free, entry, 2, wkly, comp, win, FA, Cup, fin...\n",
      "3    [U, dun, say, early, hor, ..., U, c, already, ...\n",
      "4    [Nah, I, n't, think, goes, usf, lives, around,...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "sample = df[\"text\"].head()\n",
    "\n",
    "sample = sample.apply(clean_text)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice two oddities here:\n",
    "- __...__ is present in the text.\n",
    "- After tokenizing __don't__ turned into __n't__.\n",
    "\n",
    "Let's look at the __n't__ first. __Do__ was removed as it is a stopword. The same happens to __haven't__, __didn't__ and so on. Now there is no reason we can't just keep __n't__, but to make it more consistent I will replace them with __not__.\n",
    "\n",
    "Now to the punctuation - __...__ was not included as __string.punctuation__ contains a string of unique punctuation symbols, so there is no __...__ in it. The __word_tokenize__ function is useful, however it does have some quirks. For example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'m\", 'a', 'WHAT', '!', '?', 'OMG..', 'I', 'wont', 'believe', 'he', 'said', 'that', '!', 'I', 'wo', \"n't\", '!', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "test_phrase = \"I'm a WHAT!? OMG.. I wont believe he said that! I won't!!!\"\n",
    "print(word_tokenize(test_phrase))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that it didn't separate __..__ after OMG. That is easy to fix if we replace __\".\"__ with \" \" before tokenizing. However there are some issues with __wont__ and __won't__. The former wasn't registered as __won't__ and even if it did, __wo__ is not a stopword, so our function __clean_text__ wouldn't get rid of it. Fixing these is a bit tedious now, so let's leave this for future iterations of our model and for now just implement the __\".\"__ and __n't__ fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [Go, jurong, point, crazy, Available, bugis, n...\n",
      "1                       [Ok, lar, Joking, wif, u, oni]\n",
      "2    [Free, entry, 2, wkly, comp, win, FA, Cup, fin...\n",
      "3        [U, dun, say, early, hor, U, c, already, say]\n",
      "4    [Nah, I, not, think, goes, usf, lives, around,...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = text.replace(\".\", \" \")\n",
    "    text = word_tokenize(text)\n",
    "    text = [w for w in text if w not in stopwords.words(\"english\")]\n",
    "    text = [w for w in text if w not in string.punctuation]\n",
    "    text = [w if w.lower() != \"n't\" else \"not\" for w in text]\n",
    "    return text\n",
    "\n",
    "sample = df[\"text\"].head()\n",
    "\n",
    "sample = sample.apply(clean_text)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could continue normalizing our text for a long time (and that tends to be most of the job when it comes to ML), but for now let's get on with making a model so we have a baseline to compare to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization ##\n",
    "\n",
    "Now we need to vectorize our data. For this we will use the most common method - __bag-of-words__.\n",
    "\n",
    "The first step is to represent each SMS text as a vector with as many dimensions as there are unique words in the whole body of text. For this we can use __sklearn__'s CountVectorizer. Let's test it on our __df.head()__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "sample = df.head()\n",
    "bow_trans = CountVectorizer(analyzer = clean_text).fit(sample[\"text\"])\n",
    "bow = bow_trans.transform(sample[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can proceede to weight and normalize the vectors using [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf). __TF__ stands for __Term Frequency__. This will divide the number of occurances of a each word in our SMS by the number of total tokens in it. __IDF__ stands for __Inverse Document Frequency__, which measures the importance of each words basen on the number of its occurances in the whole body of text. Let's apply it to our __bag-of-words__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_trans = TfidfTransformer().fit(bow)\n",
    "tfidf = tfidf_trans.transform(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we can fit the data to our NaiveBayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model_nb = MultinomialNB().fit(tfidf, sample[\"class\"].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model ##\n",
    "\n",
    "Before training the data we should first split our data into a training and testing set. SciKit Learn has a built in function for splitting data to a training and testing sets. Here is an example of how we get a train/test set where the test set is 30% of the total data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df['text'],df['class'],test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can repeat the steps above on our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_trans = CountVectorizer(analyzer = clean_text).fit(x_train)\n",
    "bow = bow_trans.transform(x_train)\n",
    "\n",
    "tfidf_trans = TfidfTransformer().fit(bow)\n",
    "tfidf = tfidf_trans.transform(bow)\n",
    "\n",
    "model_nb = MultinomialNB().fit(tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn has a built in __Pipeline__ function, which can accomplish the same in just 2 lines. So let's use that instead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('bow', CountVectorizer(analyzer=<function clean_text at 0x7f004f3e17b8>,\n",
       "        binary=False, decode_error='strict', dtype=<class 'numpy.int64'>,\n",
       "        encoding='utf-8', input='content', lowercase=True, max_df=1.0,\n",
       "        max_features=None, min_df=1, ngram_range=(1, 1), preprocessor=None...f=False, use_idf=True)), ('classifier', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([(\"bow\", CountVectorizer(analyzer = clean_text)), (\"tfidf\", TfidfTransformer()), (\"classifier\", MultinomialNB())])\n",
    "pipeline.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model ##\n",
    "\n",
    "Now we can test the model on our test sets and see how well it did with the __classification_report__ function baked into SciKit Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       1.00      0.95      0.97      1512\n",
      "        spam       0.67      1.00      0.80       160\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      1672\n",
      "   macro avg       0.83      0.97      0.89      1672\n",
      "weighted avg       0.97      0.95      0.96      1672\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "predictions = pipeline.predict(x_test)\n",
    "print(classification_report(predictions,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go over the results:\n",
    "- [Precision](https://en.wikipedia.org/wiki/Precision_and_recall) is the ratio of relevant instances divided by recognised instances. Let's look at __spam precison__. We can see that out of a 100 SMS our model predicted were spam 67 were actually spam.\n",
    "- [Recall](https://en.wikipedia.org/wiki/Precision_and_recall) is the ratio of recognised instances over the total number of instances. Looking at __spam recall__ we can see that our model found all of the existing spam messages.\n",
    "- [F1-score](https://en.wikipedia.org/wiki/F1_score) is a metric combined metric of __precision__ and __recall__.\n",
    "- __Support__ is the number of instances of that class\n",
    "\n",
    "Looking at the metrics we can see that our model sometimes mistakes ham for spam.\n",
    "\n",
    "If we wanted to improve our model we would want to return to the data pre-processing step and tweak the data there. Ideally we would also acquire more data to feed the model as well, which should increase the accuracy. But for now we are done with this exercise.\n",
    "\n",
    "Thank you for your time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
